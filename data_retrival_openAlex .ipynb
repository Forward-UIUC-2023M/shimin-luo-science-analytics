{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "with open('faculty.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "\n",
    "# Parse JSON data\n",
    "data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check to see if the same faculty member works in different schools\n",
    "faculty_map = {}\n",
    "cnt = 0\n",
    "for element in data:\n",
    "    person = element['name']\n",
    "    institution = element['affiliation']['id']\n",
    "    # print(str(person)+ \" \" + str(institution))\n",
    "    flag = True\n",
    "    if person in faculty_map:\n",
    "        if faculty_map[person] == institution:\n",
    "            flag = False\n",
    "    if flag:\n",
    "        faculty_map[person] = institution\n",
    "    else:\n",
    "        cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save faculty and institution information\n",
    "faculty_map = {}\n",
    "# clean the names\n",
    "for element in data:\n",
    "    person = element['name'].strip().replace(\", Ph.D.\", \"\").replace(\", M.S.\", \"\").replace(\", PhD\", \"\").replace(\"Dr. \", \"\").replace(\"Mr. \", \"\").replace('&', ' ')\n",
    "    institution = element['affiliation']['name']\n",
    "    faculty_map[person] = institution\n",
    "\n",
    "csv_file_path = 'faculty.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for key, value in faculty_map.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(\"faculty.csv has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Process institution information\n",
    "df_institution = pd.DataFrame(columns=['id', 'name', 'photoUrl'])\n",
    "\n",
    "for element in data:\n",
    "    institution = element['affiliation']\n",
    "    new_row = {'id': institution['id'], 'name': institution['name'], 'photoUrl': institution['photoUrl']}\n",
    "    if new_row['id'] not in df_institution['id'].values:\n",
    "        df_institution = pd.concat([df_institution, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# df_institution.to_csv('institution.csv', index=False)\n",
    "# df_institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the Institution name to OpemAlex format\n",
    "institution_url = 'https://api.openalex.org/institutions?search={}'\n",
    "institution_map = {}\n",
    "output_file = 'institution_map.txt'\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    for index, row in df_institution.iterrows():\n",
    "        name = row['name']\n",
    "        file.write(\"search \" + str(index) +\": \" + name+ \"\\n\")\n",
    "        url = institution_url.format(name)\n",
    "        page_with_results = requests.get(url).json()\n",
    "\n",
    "        # store standard institution names and id\n",
    "        results = page_with_results['results']\n",
    "        # If the name of an organization is the same in the json file as it is in OpenAlex\n",
    "        if len(results) > 0 and name == results[0]['display_name']:\n",
    "            file.write(name+ \"\\n\")\n",
    "            file.write(\"id: \" + results[0]['id'] + \"\\n\")\n",
    "            # institution_map[name] = name\n",
    "            institution_map[name] = []\n",
    "            institution_map[name].append(results[0]['id'])\n",
    "            file.write(\"\\n\")\n",
    "            continue\n",
    "        file.write(\"======\\n\")\n",
    "        # If the name is not exactly the same\n",
    "        for i,res in enumerate(results):\n",
    "            res_institution = res['display_name']\n",
    "            institution_map[name] = []\n",
    "            if name == 'University of Michigan':\n",
    "                institution_map[name].append(res['id'])\n",
    "                file.write(res_institution + ' ' + res['id'] + \"\\n\")\n",
    "            elif i == 0:\n",
    "                institution_map[name].append(res['id'])\n",
    "                file.write(res_institution  + ' ' + res['id'] + \"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "print(len(institution_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Manually add missing data\n",
    "institution_map['College of William Mary'] = ['https://openalex.org/I16285277']\n",
    "institution_map['University of illinois at Urbana Champaign'] = ['https://openalex.org/I157725225']\n",
    "institution_map['University at Buffalo--SUNY'] = ['https://openalex.org/I63190737']\n",
    "institution_map['Stony Brook University--SUNY'] = ['https://openalex.org/I59553526']\n",
    "institution_map['University of Minnesota--Twin Cities'] = ['https://openalex.org/I130238516']\n",
    "institution_map['University of Minnesota--Provo'] = ['https://openalex.org/I130238516']\n",
    "institution_map['Pennsylvania State University--University Park'] = ['https://openalex.org/I130769515']\n",
    "institution_map['Binghamton University--SUNY'] = ['https://openalex.org/I123946342']\n",
    "institution_map['Rutgers University--New Brunswick'] = ['https://openalex.org/I102322142','https://openalex.org/I195342982']\n",
    "institution_map['University of Pittsburgh--Pittsburgh Campus'] = ['https://openalex.org/I170201317']\n",
    "institution_map['Brigham Young University--Provo'] = ['https://openalex.org/I100005738','https://openalex.org/I43943889','https://openalex.org/I110092051']\n",
    "print(len(institution_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Manually add missing data\n",
    "# institution_map['College of William Mary'] = 'William & Mary'\n",
    "# institution_map['University of illinois at Urbana Champaign'] = 'University of Illinois Urbana-Champaign'\n",
    "# institution_map['University at Buffalo--SUNY'] = 'University at Buffalo, State University of New York'\n",
    "# institution_map['Stony Brook University--SUNY'] = 'Stony Brook University'\n",
    "# institution_map['University of Minnesota--Twin Cities'] = 'University of Minnesota'\n",
    "# institution_map['University of Minnesota--Provo'] = 'University of Minnesota'\n",
    "# institution_map['Pennsylvania State University--University Park'] = 'Pennsylvania State University'\n",
    "# institution_map['Binghamton University--SUNY'] = 'Binghamton University'\n",
    "# institution_map['Rutgers University--New Brunswick'] = 'Rutgers, The State University of New Jersey'\n",
    "# institution_map['University of Pittsburgh--Pittsburgh Campus'] = 'University of Pittsburgh'\n",
    "# # 特殊处理下密歇根\n",
    "# institution_map['University of Michigan'] = 'University of Michigan'\n",
    "# print(len(institution_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Process and store institution node information\n",
    "institution_info = pd.DataFrame(columns=['id', 'ror', 'name','relevance_score', 'country_code'])\n",
    "institution_url = 'https://api.openalex.org/institutions?search={}'\n",
    "institution_id = {}\n",
    "\n",
    "for key, value in institution_map.items():\n",
    "    url = institution_url.format(value.replace('&', ' '))\n",
    "    print('\\n' + url)\n",
    "    page_with_results = requests.get(url).json()\n",
    "\n",
    "    results = page_with_results['results']\n",
    "    if len(results) > 0:\n",
    "        res = results[0]\n",
    "        new_row = {'id': res['id'], 'ror': res['ror'], 'name': res['display_name'], 'relevance_score': res['relevance_score'], 'country_code': res['country_code']}\n",
    "        institution_info = pd.concat([institution_info, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        institution_id[res['display_name']] = res['id']\n",
    "institution_info.to_csv('institution_info.csv', index=False)\n",
    "print(\"create institution_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(find_cnt)\n",
    "# author_url_with_page = 'https://api.openalex.org/authors?search={}&select=id,display_name,last_known_institution,works_api_url&page={}'\n",
    "#\n",
    "# output_file = \"find_info.txt\"\n",
    "# with open(output_file, 'w') as file:\n",
    "#\n",
    "#     for author, institution in faculty_map.items():\n",
    "#         if author in is_find:\n",
    "#             print(\"continue\")\n",
    "#             continue\n",
    "#         # file.write((author+\" | \"+ institution+\"\\n\")\n",
    "#         # string = author + \" | \" + institution + \"\\n\"\n",
    "#         # encoded_string = string.encode('utf-8')  # 转换为utf-8编码\n",
    "#         # file.write(encoded_string.decode('utf-8'))\n",
    "#\n",
    "#         print(author+\" \"+ institution)\n",
    "#         page = 1\n",
    "#         has_more_pages = True\n",
    "#         not_stop = True\n",
    "#\n",
    "#         # loop through pages\n",
    "#         while has_more_pages and not_stop:\n",
    "#\n",
    "#             # set page value and request page from OpenAlex\n",
    "#             url = author_url_with_page.format(author,page)\n",
    "#             print(url)\n",
    "#             # file.write(url+\"\\n\")\n",
    "#             # string = url+\"\\n\"\n",
    "#             # encoded_string = string.encode('utf-8')  # 转换为utf-8编码\n",
    "#             # file.write(encoded_string.decode('utf-8'))\n",
    "#\n",
    "#             page_with_results = requests.get(url).json()\n",
    "#\n",
    "#             # loop through partial list of results\n",
    "#             results = page_with_results['results']\n",
    "#             flag = False\n",
    "#             for i,res in enumerate(results):\n",
    "#                 if res['last_known_institution'] is None:\n",
    "#                     continue\n",
    "#                 res_institution = res['last_known_institution']['display_name']\n",
    "#                 # print(res_institution)\n",
    "#                 # print(institution_map[institution])\n",
    "#                 if res_institution.find(institution_map[institution]) != -1:\n",
    "#                     new_row = {'author': author, 'institution': res['last_known_institution']['id'], 'works_url': res['works_api_url']}\n",
    "#                     whole_info = pd.concat([whole_info, pd.DataFrame([new_row])], ignore_index=True)\n",
    "#                     print(\"find!\")\n",
    "#                     file.write(\"find!\"+\"\\n\")\n",
    "#                     print(res['works_api_url'])\n",
    "#                     file.write(res['works_api_url']+\"\\n\")\n",
    "#                     flag = True\n",
    "#                     find_cnt = find_cnt + 1\n",
    "#                     is_find.append(author)\n",
    "#                     break\n",
    "#             if flag:\n",
    "#                 break\n",
    "#             # next page\n",
    "#             page += 1\n",
    "#\n",
    "#             # end loop when either there are no more results on the requested page\n",
    "#             # or the next request would exceed 10,000 results\n",
    "#             per_page = page_with_results['meta']['per_page']\n",
    "#             has_more_pages = len(results) == per_page\n",
    "#             not_stop = page < 40\n",
    "#             if ~not_stop:\n",
    "#                 is_find.append(author)\n",
    "#         print()\n",
    "#         file.write(\"\\n\")\n",
    "#     print(find_cnt)\n",
    "#     file.write(\"find: \"+find_cnt+\"\\n\")\n",
    "# whole_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "find_cnt = 0\n",
    "whole_info = pd.DataFrame(columns=['author', 'institution', 'works_url'])\n",
    "is_find = []\n",
    "not_found = pd.DataFrame(columns=['author', 'institution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(find_cnt)\n",
    "\n",
    "# use \"name\" and \"institution_id\" to find \"work_url\"\n",
    "author_url_with_page = 'https://api.openalex.org/authors?search={}&filter=last_known_institution.id:{}&select=id,display_name,last_known_institution,works_api_url&per-page=200'\n",
    "output_file = 'faculty_find_result'\n",
    "\n",
    "with open(output_file, 'a', encoding='utf-8') as file:\n",
    "    for author, institution in faculty_map.items():\n",
    "        # if the author has been searched\n",
    "        if author in is_find:\n",
    "            print(\"continue\")\n",
    "            continue\n",
    "        flag = False\n",
    "\n",
    "        print(author+ ' | ' + institution)\n",
    "        for in_id in institution_map[institution]:\n",
    "            # set page value and request page from OpenAlex\n",
    "            url = author_url_with_page.format(author,in_id)\n",
    "            print(url)\n",
    "            total_cnt = total_cnt + 1\n",
    "\n",
    "            page_with_results = requests.get(url).json()\n",
    "            # loop through partial list of results\n",
    "            results = page_with_results['results']\n",
    "            is_find.append(author)\n",
    "            if len(results) > 0:\n",
    "                print(author +' | ' +  in_id + ' | ' + str(len(results)))\n",
    "                file.write( author +' | ' +  in_id + ' | ' + str(len(results))+'\\n' )\n",
    "                for res in results:\n",
    "                    if res['last_known_institution'] is not None:\n",
    "                        new_row = {'author': author, 'institution': res['last_known_institution']['id'], 'works_url': res['works_api_url']}\n",
    "                        whole_info = pd.concat([whole_info, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                        print(\"find!\")\n",
    "                        print(res['works_api_url'])\n",
    "                        find_cnt = find_cnt + 1\n",
    "                        flag = True\n",
    "                        break\n",
    "        if ~flag:\n",
    "            file.write('Not found: ' + author)\n",
    "            new_row = {'author': author, 'institution': institution}\n",
    "            not_found = pd.concat([not_found, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        print()\n",
    "        file.write(\"\\n\")\n",
    "    print(find_cnt)\n",
    "    file.write(\"find: \"+str(find_cnt)+\"\\n\")\n",
    "whole_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "whole_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "whole_info.to_csv('whole_info.csv', index=False)\n",
    "print(total_cnt)\n",
    "print(find_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use \"work_url\" to find the certain publication\n",
    "works_url_with_page = '{}&select=id,doi,title,publication_date&page={}&per-page=200'\n",
    "publication_set = set()\n",
    "\n",
    "# use to crate relation between \"faculty\" and \"publication\" node in Neo4j\n",
    "author_works = {}\n",
    "# use to crate relation between \"faculty\" and \"institution\" node in Neo4j\n",
    "author_institution = pd.DataFrame(columns=['author_id', 'institution_id'])\n",
    "\n",
    "# information of \"publication\" node in Neo4j\n",
    "publication_info = pd.DataFrame(columns=['id', 'doi', 'title', 'publication_date'])\n",
    "# information of \"faculty\" node in Neo4j\n",
    "author_info = pd.DataFrame(columns=['id', 'name', 'works_count'])\n",
    "\n",
    "output_file = \"find_work_info.txt\"\n",
    "with open(output_file, 'w') as file:\n",
    "    for index, row in whole_info.iterrows():\n",
    "        works_url = row['works_url']\n",
    "        author = works_url.split(':')[-1]\n",
    "\n",
    "        new_row = {'author_id': works_url.split(':')[-1], 'institution_id': row['institution']}\n",
    "        # store relation between \"faculty\" and \"institution\" node\n",
    "        author_institution = pd.concat([author_institution, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        file.write(author+\"\\n\"+ works_url+\"\\n\")\n",
    "        print(author+\"\\n\"+ works_url)\n",
    "        page = 1\n",
    "        has_more_pages = True\n",
    "        work_count = 0\n",
    "        # loop through pages\n",
    "        while has_more_pages:\n",
    "\n",
    "            # set page value and request page from OpenAlex\n",
    "            url = works_url_with_page.format(works_url,page)\n",
    "            print(url)\n",
    "            file.write(url+\"\\n\")\n",
    "            page_with_results = requests.get(url).json()\n",
    "            work_count = page_with_results['meta']['count']\n",
    "\n",
    "            # loop through partial list of results\n",
    "            results = page_with_results['results']\n",
    "            if len(results) > 0:\n",
    "                author_works[author] = []\n",
    "            # flag = False\n",
    "            for i,res in enumerate(results):\n",
    "                my_id = res['id']\n",
    "                # store relation between \"faculty\" and \"publication\" node in Neo4j\n",
    "                author_works[author].append(my_id)\n",
    "                if my_id not in publication_set:\n",
    "                    publication_set.add(my_id)\n",
    "                    new_row = {'id': res['id'], 'doi': res['doi'], 'title': res['title'], 'publication_date': res['publication_date']}\n",
    "                    # store publication info\n",
    "                    publication_info = pd.concat([publication_info, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # next page\n",
    "            page += 1\n",
    "            per_page = page_with_results['meta']['per_page']\n",
    "            has_more_pages = len(results) == per_page\n",
    "        print()\n",
    "        file.write(\"\\n\")\n",
    "        new_row = {'id': author, 'name': row['author'], 'works_count': work_count}\n",
    "        author_info = pd.concat([author_info, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "publication_info.to_csv('publication_info.csv', index=False)\n",
    "author_info.to_csv('author_info.csv', index=False)\n",
    "author_institution.to_csv('author_institution.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load into csv file\n",
    "csv_file_path = 'author_works.csv'\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for key, value in author_works.items():\n",
    "        for work in value:\n",
    "            writer.writerow([key, work])\n",
    "\n",
    "print(\"create：\", csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in Neo4j \n",
    "load csv from 'file:/// author_info.csv' as line\n",
    "create(:author{id:line[0],name:line[1], works_count:line[2]})\n",
    "\n",
    "load csv from 'file:/// institution_info.csv' as line\n",
    "create(:institution {id:line[0],ror:line[1], name:line[2], relevance_score: line[3],  country_code: line[4]})\n",
    "\n",
    "load csv from 'file:/// publication_info.csv' as line\n",
    "create(:publication {id:line[0],doi:line[1], title:line[2], publication_date: line[4]})\n",
    "\n",
    "\n",
    "\n",
    "load csv from 'file:/// author_institution.csv' as line\n",
    "match(a: author {id:line[0]}) match(b: institution {id:line[1]}) \n",
    "create(a)-[r:work_in]->(b) return r;\n",
    "\n",
    "load csv from 'file:/// author_works.csv' as line\n",
    "match(a: author {id:line[0]}) match(b: publication {id:line[1]}) \n",
    "create(a)-[r: publish]->(b) return r;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
